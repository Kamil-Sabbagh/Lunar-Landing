{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Lunar Landing in Discret Invornment\n",
    "\n",
    "This Jupyter notebook documents the implementation and training of an Actor-Critic model, applied to the \"Lunar Lander\" environment from OpenAI's Gym. The model architecture is defined, trained through reinforcement learning, and finally, its performance is visually demonstrated by creating GIFs of the landing sequences. The notebook is divided into three main sections, each corresponding to a Python script that builds upon the last, illustrating the development and testing of a reinforcement learning policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "In this section, we define the ActorCritic class, a neural network model using PyTorch that predicts both actions and their value estimates given environmental states. The model is equipped with methods for performing forward passes, calculating loss based on rewards, and memory clearing for training iterations.\n",
    "\n",
    "### Code Description:\n",
    "\n",
    "* ActorCritic class initialization with layers for action and value estimation.\n",
    "* Forward pass method for determining actions based on the state input.\n",
    "* Loss calculation using discounted rewards and policy gradients.\n",
    "* Memory management methods to clear lists of rewards, log-probabilities, and state values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.affine = nn.Linear(8, 128)\n",
    "        \n",
    "        self.action_layer = nn.Linear(128, 4)\n",
    "        self.value_layer = nn.Linear(128, 1)\n",
    "        \n",
    "        self.logprobs = []\n",
    "        self.state_values = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = F.relu(self.affine(state))\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        action_probs = F.softmax(self.action_layer(state), dim=-1)\n",
    "        action_distribution = Categorical(action_probs)\n",
    "        action = action_distribution.sample()\n",
    "        \n",
    "        self.logprobs.append(action_distribution.log_prob(action))\n",
    "        self.state_values.append(state_value)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def calculateLoss(self, gamma=0.99):\n",
    "        \n",
    "        # calculating discounted rewards:\n",
    "        rewards = []\n",
    "        dis_reward = 0\n",
    "        for reward in self.rewards[::-1]:\n",
    "            dis_reward = reward + gamma * dis_reward\n",
    "            rewards.insert(0, dis_reward)\n",
    "                \n",
    "        # normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std())\n",
    "        \n",
    "        loss = 0\n",
    "        for logprob, value, reward in zip(self.logprobs, self.state_values, rewards):\n",
    "            advantage = reward  - value.item()\n",
    "            action_loss = -logprob * advantage\n",
    "            value_loss = F.smooth_l1_loss(value, reward)\n",
    "            loss += (action_loss + value_loss)   \n",
    "        return loss\n",
    "    \n",
    "    def clearMemory(self):\n",
    "        del self.logprobs[:]\n",
    "        del self.state_values[:]\n",
    "        del self.rewards[:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Actor-Critic Model\n",
    "Here, we focus on training the ActorCritic model using data from the Lunar Lander environment. The script sets up the environment, initializes the model, and contains the main training loop. Actions are selected through the policy, and the model is updated based on the calculated gradients from the loss function.\n",
    "\n",
    "### Code Description:\n",
    "\n",
    "* Environment setup and policy initialization.\n",
    "* Training loop for running episodes, collecting rewards, and updating the model.\n",
    "* Conditional logic for rendering the environment and saving model checkpoints based on performance criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LunarLander' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m             running_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m betas \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m)\n\u001b[1;32m     18\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m()\n\u001b[1;32m     21\u001b[0m policy \u001b[38;5;241m=\u001b[39m ActorCritic()\n\u001b[1;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(policy\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LunarLander' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "def train():\n",
    "    # Defaults parameters:\n",
    "    #    gamma = 0.99\n",
    "    #    lr = 0.02\n",
    "    #    betas = (0.9, 0.999)\n",
    "    #    random_seed = 543\n",
    "\n",
    "    render = False\n",
    "    gamma = 0.99\n",
    "    lr = 0.02\n",
    "    betas = (0.9, 0.999)\n",
    "    \n",
    "    \n",
    "    env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "    \n",
    "    policy = ActorCritic()\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr, betas=betas)\n",
    "    print(lr,betas)\n",
    "    \n",
    "    running_reward = 0\n",
    "    for i_episode in range(0, 10000):\n",
    "        state = env.reset()\n",
    "        for t in range(10000):\n",
    "            action = policy(state)\n",
    "            info = env.step(action)\n",
    "            state, reward, done = info[0], info[1], info[2]\n",
    "            policy.rewards.append(reward)\n",
    "            running_reward += reward\n",
    "            if render and i_episode > 1000:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "                    \n",
    "        # Updating the policy :\n",
    "        optimizer.zero_grad()\n",
    "        loss = policy.calculateLoss(gamma)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        policy.clearMemory()\n",
    "        \n",
    "        # saving the model if episodes > 999 OR avg reward > 200 \n",
    "        #if i_episode > 999:\n",
    "        #    torch.save(policy.state_dict(), './preTrained/LunarLander_{}_{}_{}.pth'.format(lr, betas[0], betas[1]))\n",
    "        \n",
    "        if running_reward > 4000:\n",
    "            torch.save(policy.state_dict(), './preTrained/LunarLander3_{}_{}_{}.pth'.format(lr, betas[0], betas[1]))\n",
    "            print(\"########## Solved! ##########\")\n",
    "            #test(name='LunarLander_{}_{}_{}.pth'.format(lr, betas[0], betas[1]))\n",
    "            break\n",
    "        \n",
    "        if i_episode % 20 == 0:\n",
    "            running_reward = running_reward/20\n",
    "            print('Episode {}\\tlength: {}\\treward: {}'.format(i_episode, t, running_reward))\n",
    "            running_reward = 0\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Visualization\n",
    "The final part of the notebook involves loading a trained model to evaluate its performance and visualizing the results through GIFs. The script tests the model in the environment, captures frames, and creates a GIF to showcase the landing sequence.\n",
    "\n",
    "### Code Description:\n",
    "\n",
    "* Loading the trained model and setting up the testing environment.\n",
    "* Running evaluation episodes, rendering frames, and saving them as images.\n",
    "* Using the saved images to create a GIF demonstrating the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ActorCritic\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "import os\n",
    "import imageio\n",
    "\n",
    "# Example usage:\n",
    "# create_gif('/path/to/image_folder', '/path/to/output/output.gif', 0.5)\n",
    "\n",
    "def test(n_episodes=5, name='LunarLander_TWO.pth'):\n",
    "    env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "    policy = ActorCritic()\n",
    "    \n",
    "    policy.load_state_dict(torch.load('./preTrained/{}'.format(name)))\n",
    "    \n",
    "    render = True\n",
    "    save_gif = True\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        running_reward = 0\n",
    "        for t in range(10000):\n",
    "            action = policy(state)\n",
    "            info = env.step(action)\n",
    "            state, reward, done = info[0], info[1], info[2]\n",
    "            running_reward += reward\n",
    "            if render:\n",
    "                 if save_gif:\n",
    "                    img = env.render()\n",
    "                    img = np.array(img)\n",
    "                    img = img[0]\n",
    "                    img = Image.fromarray(img)\n",
    "                    os.makedirs(f'./solutions/attepmt3', exist_ok=True) \n",
    "                    img.save(f'./solutions/attepmt3/{t}.jpg')\n",
    "            if done:\n",
    "                break\n",
    "        print('Episode {}\\tReward: {}'.format(i_episode, running_reward))\n",
    "    env.close()\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    test(name=\"LunarLander_0.02_0.9_0.999.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: Pillow 9.0.1\n",
      "Uninstalling Pillow-9.0.1:\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 816, in move\n",
      "    os.rename(src, real_dst)\n",
      "PermissionError: [Errno 13] Permission denied: '/usr/lib/python3/dist-packages/PIL' -> '/tmp/pip-uninstall-umx9tu82'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kamil/.local/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"/home/kamil/.local/lib/python3.10/site-packages/pip/_internal/commands/uninstall.py\", line 105, in run\n",
      "    uninstall_pathset = req.uninstall(\n",
      "  File \"/home/kamil/.local/lib/python3.10/site-packages/pip/_internal/req/req_install.py\", line 727, in uninstall\n",
      "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
      "  File \"/home/kamil/.local/lib/python3.10/site-packages/pip/_internal/req/req_uninstall.py\", line 380, in remove\n",
      "    moved.stash(path)\n",
      "  File \"/home/kamil/.local/lib/python3.10/site-packages/pip/_internal/req/req_uninstall.py\", line 271, in stash\n",
      "    renames(path, new_path)\n",
      "  File \"/home/kamil/.local/lib/python3.10/site-packages/pip/_internal/utils/misc.py\", line 358, in renames\n",
      "    shutil.move(old, new)\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 834, in move\n",
      "    rmtree(src)\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "PermissionError: [Errno 13] Permission denied: 'IcoImagePlugin.py'\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (9.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall Pillow -y\n",
    "! pip install Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF created at ./solutions/output.gif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def create_gif_with_pillow(image_folder, output_path, duration=500):\n",
    "    \"\"\"\n",
    "    Create a GIF from a sequence of images in a folder using Pillow.\n",
    "\n",
    "    Parameters:\n",
    "    - image_folder: str, the path to the folder containing images.\n",
    "    - output_path: str, the path where the GIF should be saved.\n",
    "    - duration: int, the duration each image is displayed in the GIF in milliseconds.\n",
    "    \"\"\"\n",
    "    # List all files in the directory and sort them\n",
    "    files = [os.path.join(image_folder, file) for file in sorted(os.listdir(image_folder)) if file.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    files = sorted(files)\n",
    "    # Load all the images\n",
    "    images = [Image.open(image) for image in files if os.path.isfile(image)]\n",
    "    \n",
    "    # Convert images to the same mode for compatibility\n",
    "    images = [image.convert('RGBA') for image in images]\n",
    "    \n",
    "    # Specify the output path for the GIF\n",
    "    gif_path = os.path.join(output_path, \"output.gif\")\n",
    "\n",
    "    # Save the images as a GIF\n",
    "    images[0].save(gif_path, save_all=True, append_images=images[1:], optimize=False, duration=duration, loop=0)\n",
    "\n",
    "    print(f\"GIF created at {gif_path}\")\n",
    "\n",
    "# Usage example:\n",
    "create_gif_with_pillow(\"./solutions/attepmt3\", \"./solutions\", duration=25)  # Duration in milliseconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This notebook provides a comprehensive look at setting up, training, and evaluating a reinforcement learning model for the Lunar Lander task. Starting from the model's architectural definition, through training with reinforcement learning techniques, to visualizing the outcomes, each step is detailed for clarity and reproducibility. This serves as a practical introduction to applying deep learning techniques in reinforcement learning environments.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
